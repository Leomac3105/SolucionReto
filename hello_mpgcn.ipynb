{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0587dc",
   "metadata": {},
   "source": [
    "# Réplica del Paper: MP‑GCN para Reconocimiento de Actividades Grupales\n",
    "\n",
    "Este cuaderno reproduce de forma simplificada las ideas principales del paper **\"Skeleton‑based Group Activity Recognition via Spatial‑Temporal Panoramic Graph\"**.  \n",
    "Dado que el conjunto de datos original (Volleyball, NBA, Kinetics) y el código oficial requieren descargas externas, aquí implementamos una versión ligera que permite instalar las dependencias, construir las formas de entrada del alimentador (feeder), definir una versión mínima del modelo MP‑GCN y ejecutar un pase hacia adelante con datos sintéticos.  \n",
    "\n",
    "> **Objetivos del cuaderno**\n",
    ">\n",
    "> 1. Instalar las dependencias esenciales (`torch`, `pyyaml`, `tqdm`, `fvcore`, `tensorboardX`).  \n",
    "> 2. Construir un conjunto de datos sintético con forma \\(X \\in [C, T, V', M]\\) donde \\(C\\) son canales (posición x,y, confianza), \\(T\\) es la longitud temporal, \\(V'\\) es el número de nodos (17 articulaciones humanas + *número de objetos*), y \\(M\\) es el número máximo de personas por secuencia.  \n",
    "> 3. Definir matrices de adyacencia para las conexiones **intra‑persona** (topología del esqueleto humano), **persona↔objeto** (manos con objetos) e **inter‑persona** (pelvis con pelvis).  \n",
    "> 4. Implementar una versión simplificada del MP‑GCN con cuatro *streams*: Joints (\\(J\\)), Bones (\\(B\\)), Joint Motion (\\(JM\\)) y Bone Motion (\\(BM\\)).  \n",
    "> 5. Ejecutar un *forward* con datos sintéticos y mostrar las formas de tensores en cada etapa.\n",
    "\n",
    "La intención es familiarizarse con el pipeline del artículo sin descargar grandes conjuntos de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install torch pyyaml tqdm tensorboardX fvcore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Parámetros\n",
    "C = 3            # canales: x, y, confidence\n",
    "T = 48           # frames por ventana\n",
    "num_joints = 17  # articulaciones humanas\n",
    "n_objects = 1     # número de objetos\n",
    "V = num_joints + n_objects  # nodos totales\n",
    "M = 4            # número máximo de personas\n",
    "\n",
    "# Datos sintéticos con forma [C, T, V, M]\n",
    "synthetic_data = torch.rand(C, T, V, M)\n",
    "\n",
    "print(f\"Forma del tensor de entrada X: {synthetic_data.shape} (C,T,V',M)\")\n",
    "\n",
    "# Matrices de adyacencia\n",
    "# A0: identidad\n",
    "A0 = torch.eye(V)\n",
    "\n",
    "# A_intra: conexiones del esqueleto y manos↔objeto\n",
    "skeleton_edges = [\n",
    "    (5,6),(5,7),(7,9),(6,8),(8,10),\n",
    "    (11,12),(5,11),(6,12),(11,13),(13,15),(12,14),(14,16)\n",
    "]\n",
    "A_intra = torch.zeros(V, V)\n",
    "for i, j in skeleton_edges:\n",
    "    A_intra[i, j] = 1\n",
    "    A_intra[j, i] = 1\n",
    "\n",
    "# Conectar manos con objeto\n",
    "obj_idx = V - 1\n",
    "for hand in [9, 10]:\n",
    "    A_intra[hand, obj_idx] = 1\n",
    "    A_intra[obj_idx, hand] = 1\n",
    "\n",
    "# A_inter: conexiones de pelvis (simplificado)\n",
    "A_inter = torch.zeros(V, V)\n",
    "A_inter[11, 11] = 1\n",
    "A_inter[12, 12] = 1\n",
    "\n",
    "print(\"\"\"Matrices de adyacencia creadas:\n",
    " - A0: identidad\n",
    " - A_intra: conexiones esqueléticas y manos↔objeto\n",
    " - A_inter: conexiones de pelvis (simplificado)\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleMPGCN(nn.Module):\n",
    "    \"\"\"Una versión mínima del modelo MP‑GCN. Aplica una convolución espacial sobre cada stream y luego fusiona los resultados.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, A0, A_intra, A_inter, num_classes):\n",
    "        super().__init__()\n",
    "        self.A0 = A0\n",
    "        self.A_intra = A_intra\n",
    "        self.A_inter = A_inter\n",
    "        # Convoluciones para cada stream\n",
    "        self.conv_J  = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1))\n",
    "        self.conv_B  = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1))\n",
    "        self.conv_JM = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1))\n",
    "        self.conv_BM = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1))\n",
    "        self.fc = nn.Linear(out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [C,T,V,M]\n",
    "        J = x\n",
    "        # Bones: diferencias entre articulaciones conectadas\n",
    "        B = torch.einsum('vw,ctwm->ctvm', self.A_intra, J) - J\n",
    "        # Joint motion\n",
    "        JM = J[:,1:] - J[:,:-1]\n",
    "        JM = F.pad(JM, (0,0,0,0,0,1))\n",
    "        # Bone motion\n",
    "        B_shift = torch.einsum('vw,ctwm->ctvm', self.A_intra, J) - J\n",
    "        BM = B_shift[:,1:] - B_shift[:,:-1]\n",
    "        BM = F.pad(BM, (0,0,0,0,0,1))\n",
    "        # Función auxiliar para aplicar conv y reducir\n",
    "        def apply_conv(conv, stream):\n",
    "            Cc, Tt, Vv, Mm = stream.shape\n",
    "            stream = stream.permute(1,3,2,0).contiguous()  # [T,M,V,C]\n",
    "            stream = stream.view(Tt, Mm*Vv, Cc).permute(2,0,1).unsqueeze(0)  # [1,C,T,VM]\n",
    "            out = conv(stream)\n",
    "            # Media sobre dimensiones temporales y nodos\n",
    "            out = out.mean(dim=-1).mean(dim=-1)\n",
    "            return out.squeeze(0)\n",
    "        out_J  = apply_conv(self.conv_J,  J)\n",
    "        out_B  = apply_conv(self.conv_B,  B)\n",
    "        out_JM = apply_conv(self.conv_JM, JM)\n",
    "        out_BM = apply_conv(self.conv_BM, BM)\n",
    "        fused = out_J + out_B + out_JM + out_BM\n",
    "        logits = self.fc(fused)\n",
    "        return logits\n",
    "\n",
    "# Inicializar y ejecutar el modelo\n",
    "num_classes = 6\n",
    "model = SimpleMPGCN(in_channels=C, out_channels=16, A0=A0, A_intra=A_intra, A_inter=A_inter, num_classes=num_classes)\n",
    "\n",
    "# Ejecución de prueba\n",
    "logits = model(synthetic_data)\n",
    "print(f\"Salida del modelo: logits de tamaño {logits.shape}\")\n",
    "print(\"Logits:\", logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bec13",
   "metadata": {},
   "source": [
    "### Discusión y próximos pasos\n",
    "\n",
    "En este cuaderno hemos:\n",
    "\n",
    "- Instalado las dependencias necesarias para ejecutar un modelo de grafos en PyTorch.\n",
    "- Definido la forma de entrada \\([C,T,V',M]\\) utilizada por MP‑GCN y generado un tensor sintético para simular datos.\n",
    "- Construido matrices de adyacencia que representan las conexiones intra‑persona, persona↔objeto e inter‑persona.\n",
    "- Implementado un modelo MP‑GCN básico que procesa cuatro *streams* (J, B, JM, BM) y genera una predicción para una de las seis etiquetas de escena.\n",
    "\n",
    "Para replicar completamente los resultados del paper original sería necesario descargar y preparar los conjuntos de datos indicados en el repositorio oficial (Volleyball, NBA, Kinetics) y entrenar el modelo con configuraciones específicas. Este ejercicio sirve para familiarizarse con la estructura de entrada y el flujo de datos del modelo antes de trabajar con datos reales.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
